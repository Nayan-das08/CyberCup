{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2362285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tflearn # tensorflow deep learning\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk # natural langauge tool kit for nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2a2d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c852d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_new.json', 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e59df18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire</td>\n",
       "      <td>[Fire, Flames, Choking, Suffocating, Burning, ...</td>\n",
       "      <td>[Please consider the following measures- \\n 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earthquake</td>\n",
       "      <td>[Earthquake, Falling, shaking, moving, cracks,...</td>\n",
       "      <td>[Please consider the following measures- \\n 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flood</td>\n",
       "      <td>[Water, Flood, flooding, submerge, drowning, r...</td>\n",
       "      <td>[Please consider the following measures- \\n 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avalanche</td>\n",
       "      <td>[Snow, avalanche, snowpack, debris, slab, runo...</td>\n",
       "      <td>[Please consider the following measures- \\n 1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tag                                           patterns  \\\n",
       "0        Fire  [Fire, Flames, Choking, Suffocating, Burning, ...   \n",
       "1  Earthquake  [Earthquake, Falling, shaking, moving, cracks,...   \n",
       "2       Flood  [Water, Flood, flooding, submerge, drowning, r...   \n",
       "3   Avalanche  [Snow, avalanche, snowpack, debris, slab, runo...   \n",
       "\n",
       "                                           responses  \n",
       "0  [Please consider the following measures- \\n 1....  \n",
       "1  [Please consider the following measures- \\n 1....  \n",
       "2  [Please consider the following measures- \\n 1....  \n",
       "3  [Please consider the following measures- \\n 1....  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting dictinary to pandas dataframe\n",
    "df = pd.DataFrame.from_dict(dataset['data'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee88c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftershock', 'avalanch', 'beacon', 'blaz', 'burn', 'chok', 'clos', 'crack', 'dam', 'debr', 'drown', 'earthquak', 'ep', 'fal', 'fir', 'flam', 'flood', 'heat', 'inferno', 'magintud', 'mov', 'prob', 'rainfal', 'richt', 'road', 'runout', 'rupt', 'seism', 'shak', 'shovel', 'slab', 'smok', 'snow', 'snowpack', 'storm', 'submerg', 'suffoc', 'swing', 'trem', 'tsunam', 'volcano', 'wat']\n"
     ]
    }
   ],
   "source": [
    "#prepare training data\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for data in dataset[\"data\"]: # dataset is dict. 'data' is key. dataset['data'] is array of dict. data is dict\n",
    "    for pattern in data[\"patterns\"]: #pattern is array element in dict element of array of dict\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        docs_x.append(w)\n",
    "        docs_y.append(data[\"tag\"])\n",
    "\n",
    "    if data[\"tag\"] not in labels:\n",
    "        labels.append(data[\"tag\"])\n",
    "\n",
    "ignore_words = ['?', '.', '!']\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "895885f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f72166fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avalanche', 'Earthquake', 'Fire', 'Flood']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9307dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fire'], ['Flames'], ['Choking'], ['Suffocating'], ['Burning'], ['Smoke'], ['heat'], ['burns'], ['blaze'], ['inferno'], ['volcano'], ['Earthquake'], ['Falling'], ['shaking'], ['moving'], ['cracks'], ['rupturing'], ['swinging'], ['aftershock'], ['tremor'], ['seismic'], ['epicenter'], ['tsunami'], ['magintude'], ['richter'], ['damage'], ['Water'], ['Flood'], ['flooding'], ['submerge'], ['drowning'], ['rainfall'], ['storm'], ['road', 'closure'], ['Snow'], ['avalanche'], ['snowpack'], ['debris'], ['slab'], ['runout'], ['beacon'], ['probe'], ['shovel']]\n"
     ]
    }
   ],
   "source": [
    "print(docs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69c1c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Fire', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Earthquake', 'Flood', 'Flood', 'Flood', 'Flood', 'Flood', 'Flood', 'Flood', 'Flood', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche', 'Avalanche']\n"
     ]
    }
   ],
   "source": [
    "print(docs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "534ccfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shak']\n"
     ]
    }
   ],
   "source": [
    "wrds = [stemmer.stem(w) for w in docs_x[13]]\n",
    "print(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "172ca0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))] # intialise the a list out_empty with 0's with len = length of labels\n",
    "\n",
    "for x, doc in enumerate(docs_x): # creates tuples of docs_x's index and values such that the index of the docs_x elements are stores in x and the element values are stored in the variable doc\n",
    "    \n",
    "\tbag = []\n",
    "    \n",
    "\twrds = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "\tfor w in words:\n",
    "\t\tif w in wrds:\n",
    "\t\t\tbag.append(1)\n",
    "\t\telse:\n",
    "\t\t\tbag.append(0)\n",
    "            \n",
    "\toutput_row = out_empty[:]\n",
    "\toutput_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "\ttraining.append(bag)\n",
    "\toutput.append(output_row)\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5cea2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(training)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6a9f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network model\n",
    "tf.compat.v1.reset_default_graph() # creates a fresh new empty graph by clearing the nodes and operations of any previous computation\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])]) # None specifies that the input layer can accept a variable number of samples., no of neurons in input layer= 32\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #  no of neurons = 3\n",
    "net = tflearn.regression(net)\n",
    "# net = tflearn.softmax_categorical_crossentropy(net,output)\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27206012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5999  | time: 0.020s\n",
      "| Adam | epoch: 1000 | loss: 0.00000 - acc: 0.9508 -- iter: 40/43\n",
      "Training Step: 6000  | time: 0.022s\n",
      "| Adam | epoch: 1000 | loss: 0.00000 - acc: 0.9557 -- iter: 43/43\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\manya\\PycharmProjects\\python\\hackathon\\model_revised1.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# neural network training\n",
    "# try:\n",
    "# \tmodel.load(\"model.tflearn\")\n",
    "# except:\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model_revised1.tflearn\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d5a4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s,words):\n",
    "\tbag = [0 for _ in range(len(words))]\n",
    "\ts_words = nltk.word_tokenize(s)\n",
    "\ts_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "\tfor se in s_words:\n",
    "\t\tfor i, w in enumerate(words):\n",
    "\t\t\tif w == se:\n",
    "\t\t\t\tbag[i] = 1\n",
    "\n",
    "\treturn np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41e213c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'speech_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8300/1736208486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyttsx3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyttsx3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sapi5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#sapi5 is microsoft speech api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import datetime\n",
    "\n",
    "engine = pyttsx3.init('sapi5') #sapi5 is microsoft speech api\n",
    "voices = engine.getProperty('voices') #main program\n",
    "engine.setProperty('voice',voices[0].id)\n",
    "\n",
    "def speak(audio):\n",
    "    engine.say(audio)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def chat():\n",
    "    print(\"Start Talking with the Bot (say quit to stop!)\")\n",
    "    while True:\n",
    "        r=sr.Recognizer()\n",
    "        inp=\"\"\n",
    "        def say():\n",
    "            with sr.Microphone() as source:\n",
    "                print(\"Listening...\")\n",
    "                r.pause_threshold = 0.5\n",
    "                audio = r.listen(source)\n",
    "            try:\n",
    "                print(\"Recognizing...\")\n",
    "                inp = r.recognize_google(audio, language='en-in')\n",
    "                print(f\"You: {inp}\\n\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Say that again please...\")\n",
    "                say()\n",
    "        say()\n",
    "        \n",
    "        if inp.lower() == \"quit\":\n",
    "            return \"quit\"\n",
    "        \n",
    "        results = model.predict(bag_of_words(inp, words).reshape(1, len(training[0]), 1))[0]\n",
    "        print(results)\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "# Find the responses for the predicted label\n",
    "        responses = None\n",
    "        for tg in dataset[\"data\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "                break\n",
    "\n",
    "# Find the response with the highest score\n",
    "        max_response = None\n",
    "        max_score = 0\n",
    "        for response in responses:\n",
    "            score = results[results_index]\n",
    "            if score > max_score:\n",
    "                max_response = response\n",
    "                max_score = score\n",
    "    \n",
    "        print(max_response)\n",
    "        speak(max_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efdcada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Talking with the Bot (type quit to stop!)\n",
      "You: I am covered in snow till my face\n",
      "Please consider the following measures- \n",
      " 1. Try to stay on the surface of the snow by swimming or using other means to stay afloat.\n",
      " 2. Try to hold onto a tree or other objects that will not be carried away by the avalanche.\n",
      " 3. Cover your nose and mouth with your hand or a piece of clothing to protect yourself from the snow.\n",
      " 4. If you are unable to move, try to stand as still as possible to conserve energy and avoid inhaling snow .\n",
      " 5. Call the emergency number 108 to seek further help.\n",
      "\n",
      "\n",
      "\n",
      "You: I am feeling dizzy\n",
      "I didn't get that, please try again ! \n",
      "You: I am choking\n",
      "Please consider the following measures- \n",
      " 1. Stay calm and do not panic \n",
      " 2. Sound the fire alarm and shout to alert others in the house \n",
      " 3. Call the local fire department immediately by dialing 101 or the emergency number in your area. Provide them with your address and details of the fire. \n",
      " 4. Close all doors and windows to contain the fire and prevent it from spreading. \n",
      " 5. Evacuate the house immediately and gather at a safe location outside.\n",
      "\n",
      "\n",
      "\n",
      "You: My bed is shaking too much\n",
      "Please consider the following measures- \n",
      " 1. Stay calm and don't panic. \n",
      " 2. If you are indoors: \n",
      " Take cover under a sturdy piece of furniture or against an interior wall. \n",
      " Stay away from windows, mirrors, and other objects that can break.\n",
      " Do not use elevators. \n",
      " 3. If you are outdoors: \n",
      " Move to an open area away from buildings, power lines, and other potential hazards.\n",
      " Avoid bridges, overpasses, and other structures that can collapse.\n",
      " 4. If you are in a vehicle:\n",
      " Pull over to a safe location and stop the car.\n",
      " Stay inside the vehicle until the shaking stops.\n",
      " 5. If you are at work or school:\n",
      " Follow the evacuation plan and procedures established by the building management. \n",
      " Do not use elevators.\n",
      " Follow the instructions of emergency personnel. \n",
      " 6. Call the emergency number 1091 to seek further help. \n",
      "\n",
      "\n",
      "\n",
      "You: I am drowning\n",
      "Please consider the following measures- \n",
      " 1. Stay informed and stay updated about the flood warning and evacuation orders issued by the local authorities, and follow their instructions.\n",
      " 2. If you are in a flood-prone area, be prepared to evacuate immediately.\n",
      " 3. If you are at home, move to the highest point in your home.\n",
      " 4. Turn off all electricity, gas, and water supplies if instructed to do so by the authorities.\n",
      " 5. Do not try to move too much water by yourself. Call the emergency number 1090 to seek professional help. \n",
      "\n",
      "\n",
      "\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "#start chatting\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "271d6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #chat interface\n",
    "# def chat():\n",
    "    \n",
    "# \tprint(\"Start Talking with the Bot (type quit to stop!)\")\n",
    "# \twhile True:\n",
    "# \t\tinp = input(\"You: \")\n",
    "# \t\tif inp.lower() == \"quit\":\n",
    "# \t\t\tbreak\n",
    "\n",
    "# \t\tresults = model.predict([bag_of_words(inp,words)])[0]\n",
    "# \t\tresults_index = np.argmax(results) # returns the index of the maximum element in a 1-d array\n",
    "# \t\ttag = labels[results_index]\n",
    "\n",
    "# \t\tif results[results_index] > 0.5:\n",
    "# \t\t\tfor tg in dataset[\"data\"]:\n",
    "# \t\t\t\tif tg['tag'] == tag:\n",
    "# \t\t\t\t\tresponses = tg['responses']\n",
    "# \t\t\tprint(random.choice(responses))\n",
    "# \t\t\tprint(\"\\n\")\n",
    "\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"I didn't get that, please try again ! \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
